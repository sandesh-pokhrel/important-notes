------------------------------------------------------------ DATABASE BASIC -------------------------------------

AGGREGATE FUNCTION -->>
	Return single value and works in a row.
	Eg: min, max, avg etc.
	
SUBQUERIES -->>
	Can be applied on SELECT, WHERE and FROM keywords
	Types
		Single Valued subqueries - Return single value (Can be applied on where = )
		Multiple Valued subqueries - Return multiple values (Can be applied on where in)
		Correlated subqueries - subqueries referencing the table or table alias from enclosing scope
		Multicolumn subqueries - subqueries returning more than one column
		Inline views - query used as a table providing the alias and performing further actions
		
INDEXES -->>
	Scanning table with millions of rows are often time consuming..
	Indexes are those schema objects which increments the performance of sql queries
	The most famous way of indexing is call B-Tree indexing. Where objects are stored in tree structure
	For say if index is create in salary column then the rows from value 1000 - 5000 will be stored in one leaf, 5000-10000 in another leaf and so on.
	Eg:
		CREATE INDEX index_name on table_name(column_name)
	Index referencing more than one column is called composite index.
	If multiple indexes are created for certain table then it will decrease the performance such that the DML operations performed should again recalculate where those value should be placed
	Explaing plan feature can be used to evaluate whether the indexes are being used or not during query execution

DATA DICTIONARY -->>
	USER_{name} - These view contains the info about the objects owned by the current connected user
	ALL_{name} - These view contains the info about the objects owned by the current user and those objects for which the user is given permission
	DBA_{name} - These view contains the info about all objects for all users
	DICT - This view has the metadata regarding all the views available as oracle data dictionary
	V${name} - These view has the dynamic info about various actions and are used primarily by adminstrators
				V$SESSION, V$SQLAREA, V$LOCK, V$INSTANCE etc.
	Useful
		show errors;
		select * from user_errors;
				
TRANSACTIONS -->>
	COMMIT -
		Commit will commit the transactions made
		Rollback will rollback the changes made after last commit
		The record that is being modified in one session and not yet been committed will be locked such that other session cannot modify it
		There is another keyword called savepoint which marks the point of transaction and the rollback can be performed upto that point.
		syntax -
			(queries.......);   savepoint s1;  (queries.......);   rollback to s1;    -- this will rollback the transaction upto the savepoint s1
	
	
SEQUENCES -->>
	Database object used to generate unique objects
	syntax - 
		create sequence seq_name minvalue 1 maxvalue 99999 start with 1 increment by 1 cache 20
		cache will load the given integer into the memory to optimize the performance
	sequence_name.currval - current value in sequence, sequence.nextval - next value in sequence
	

IDENTITY COLUMNS -->>
	These column can be thought as built in columns with embedded sequence
	Types -
		GENERATED ALWAYS AS IDENTITY     -    If custom value is inserted then it will throw error
		GENERATED BY DEFAULT AS IDENTITY    - If custom value is inserted then it will accept .. the value is only inserted as default if not specified
		GENERATED BY DEFAULT AS IDENTITY (START WITH 100 INCREMENT BY 100) 
		
TRIGGERS -->>
	Triggers execute themselves at certain system event
	Types 
		DML
		DDL
		System Events (startup, shutdown, error messages)
		User Events (logon, logoff etc)
	
SYNONYMS -->>
	Synonyms are alternative names for database objects like table, views etc.
	syntax	
		create or replace synonym syn_name for (table_name/view_name etc).
		create or replace public synonym syn_name for (table_name/view_name etc).
		
		
PARTITIONING -->>
	Allows oracle data to be subdivided in small parts
	Tables are split horizontally
	Rows are divided and assigned to certain partitions based upon the condition on certain columns.
	Partitioning helps optimizer to find out in which partition the data reside and omit other partition from scanning.
	Partition vs Index
		If both are used then database optimizer might use index over partition.
	Types of table partitions
		List Partitioning -
			Here we have made four partitions. Values provides the condition analyzing which the records will be assigned to partition
			CREATE TABLE table_name 
			(columns.......)
			PARTITION BY LIST (column_name) (
				PARTITION parname_1 VALUES (...),
				PARTITION parname_2 VALUES (...),
				PARTITION parname_3 VALUES (...),
				PARTITION parname_4 VALUES (DEFAULT)    -- Value not matching any will be assigned here
			);
			
			Splitting existing partition parname_4 into two partitions
			ALTER TABLE table_name SPLIT PARTITION parname_4 VALUES (....)
			INTO (PARTITION parname_5, PARTITION parname_4);
		Range Partitioning -
			Here we have made four partitions. Values provides the condition analyzing which the records will be assigned to partition
			CREATE TABLE table_name 
			(columns.......)
			PARTITION BY RANGE (column_name) (
				PARTITION parname_1 VALUES LESS THAN (<something>),
				PARTITION parname_2 VALUES LESS THAN (<something>),
				PARTITION parname_3 VALUES LESS THAN (<something>),
				PARTITION parname_4 VALUES LESS THAN (<something>) 
			);
			
			Adding new partition
			ALTER TABLE table_name ADD PARTITION parname_5 VALUES LESS THAN (<something>)
		Hash Partitioning - 
			This method of partitioning is used when there is no specific way to partition the table.
			It just analyzes hash value and perform partitioning
			CREATE TABLE table_name 
			(columns......., PARTITION BY HASH (col_name)) PARTITIONS 4;
			
			ANALYZE TABLE table_name COMPUTE STATISTICS;   -- This will analyze the table and assign record to specific partition
			SELECT table_name, partition_name, num_rows FROM user_tab_partitions WHERE table_name = 'TABLE_NAME';
					-- This will list all partitions and provide info like number of rows etc.
		Interval Partitioning - 
		Composite Partitioning - 
		Automatic list Partitioning - 
		
DIRECTORY -->>
	create directory dir_name as 'location';
		directory cannot be renamed
	any LOB file with size greater than 4000 byte will be stored in directory
	
SQL LOADER -->>
	It is used to load the content of file in the table
	Data is loaded permanently
	Useful (eg.)
		control file -
			OPTIONS (SKIP=1) -- skip the header i.e, first row (if not required the remove this line)
			LOAD DATA
			INFILE 'student.csv' -- name of the text file containing data (if this is in sqlldr as data=student.csv the this infile line can be omitted)
			APPEND  -- append , truncate
			INTO TABLE STUDENT
			fields terminated by ","
			(
				id,
				name
			)
			
			
			OPTIONS (SKIP=1)
			LOAD DATA          -- here infile is not mentioned as data param is mentioned through sqlldr
			TRUNCATE
			INTO TABLE STUDENT
			WHEN ID <> '5000'
			FIELDS TERMINATED BY ","
			(
				id,
				name
			)
			
			
			OPTIONS (SKIP=1)
			LOAD DATA       
			INFILE 'student.csv'
			BADFILE 'student_bad.csv'
			DISCARDFILE 'student_discard.dsc'    -- here all params is mentioned so only control=student.ctl can be mentioned in sqlldr      
			TRUNCATE
			INTO TABLE STUDENT
			WHEN ID <> '5000'
			FIELDS TERMINATED BY ","
			(
				id,
				name
			)
			
			
		calling through terminal
			sqlldr userid=sandesh/sandesh control=student.ctl data=student.csv bad=student_bad.bad log=student_log.log
			sqlldr userid=sandesh/sandesh control=student.ctl data=student.csv bad=student_bad.bad log=student_log.log discard=student_discard.dsc
				--either provide data, control, bad, discard file in sqlldr command or in the control file
	
EXTERNAL TABLES -->>
	Data won't be stored inside datbase
	Data is stored in files
	Similar concept as sqlloader is brought in the external table
		Control file is not present here like in sqlloader
	Data is not loaded permanently like sql loader
	Directory name, datafile, field seperator and record seperator is required for this
	Directory student_directory is created and the read write access was given to the user
	Using data pump
		create table student_data_pump
		organization external
		(
		type oracle_datapump
		default directory student_directory
		location ('student_pump.dmp'))
		as select * from student;
		
		here dmp file is created, student_data_pump table is created which points to the student_pump.dmp file
			if dump file is deleted then the table will be invalid, it won't display any data
		
	Using oracle loader
		create table student_external
		(
			id number, name varchar2(100)
		)
		organization external   -- specifies that it is external table
		(
		type oracle_loader  -- datapump can be used if the data is to be exported to file from the table (example mentioned above)
		default directory student_directory   -- default directory, if not specified in location() then this dir will be used
		access parameters
		(
		records delimited by newline
		badfile 'ext_student.bad'
		discardfile 'ext_discard_student.dsc'
		logfile 'ext_student.log'
		skip 1
		fields terminated by ','
		--reject rows with all null fields
		missing field values are null
		(id, name)   -- optional (fields)
		)
		location ('student.csv')      -- default directory is scanned, for using other explicitly use location(another_dir_name:'student.csv')
		)
		reject limit unlimited  -- optional
		;
	
		records delimited and field terminated are compulsary
		NOTE: remove all those comments inside query otherwise it will throw error (data catridge error)
		
		
TABLE LOCKS -->>
	Types - DML, DDL, Internal
	Shared locks are shared and can be acquired by multiple users at a time whereas exclusive locks can only be acquired by a single user
	Important
			S	X
		S	yes yes
		
		X	yes no
		Exclusive exclusive is not allowed
		This means: reader won't block reader, reader won't block writer, writer won't block reader but writer will block writer
	Lock modes - Share / Row share, Share update, Exclusive / Row Exclusive / Share Row Exclusive
		Share / Row share - Multiple users
		Share update - Multiple users
		Exclusive / Share Row Exclusive - Single user (specifify NOWAIT such that it won't wait for lock if already locked by other users)
		Row Exclusive - Multiple users
	Row level implicit locks
		These are the implicit locks automatically placed by database
		When one session performs any DML in one row then other session and users cannot modify it unless committed or rollbacked
	Explicit locks
		Different lock modes are used for the explicit locks
		Those modes are used for explicitly locking the tables in different way
	Practical
		lock table student in share mode; 
				-- can be acquirred by multiple users
				-- if the lock is acquired by only one user then it can modify the table. Other user cannot modify
				-- if the lock is with more than one then no one can modify the table only read is possible
		lock table student in share update mode;
				-- can be acquirred by multiple users
				-- if the lock is with more than one then also the table can be modified, but same row cannot me modified 
		lock table sandesh.student in exclusive mode nowait;
				-- can be acquirred by only one user
				-- if any type of lock is present with that object then exclusive lock cannot be made
				-- other user can read the data but cannot perform any manipulation
		lock table sandesh.student in row exclusive mode;
				-- can be acquirred by only one user
				-- if the lock is with more than one then also the table can be modified, but same row cannot me modified 
				
		SHARE
			SHARE permits concurrent queries but prohibits updates to the locked table.
		ROW SHARE
			ROW SHARE permits concurrent access to the locked table but prohibits users from locking the entire table for exclusive access. 
			ROW SHARE is synonymous with SHARE UPDATE, which is included for compatibility with earlier versions of Oracle Database.
		SHARE UPDATE
			See ROW SHARE.
		EXCLUSIVE
			EXCLUSIVE permits queries on the locked table but prohibits any other activity on it.
		ROW EXCLUSIVE
			ROW EXCLUSIVE is the same as ROW SHARE, but it also prohibits locking in SHARE mode. 
			ROW EXCLUSIVE locks are automatically obtained when updating, inserting, or deleting.
		SHARE ROW EXCLUSIVE
			SHARE ROW EXCLUSIVE is used to look at a whole table and to allow others to look at rows in the table but 
				to prohibit others from locking the table in SHARE mode or from updating rows.
				
PSEUDO-COLUMNS -->>
	Columns that are automatically present in oracle table
	These columns are not manipulated by the user
	Five types of pseudo columns
		rownum, rowid, level, currval, nextval
		currval, nextval is for sequences
		rowid, rownum, level is for tables
			rowid is the physical address
			rownum is just the sequential numbers
			level used in hierarchical report used in connect by clause
				connect by displays hierarchical records i.e, in tree structure
				level will display who belongs to the top etc.
			

GENERATING OUTPUT FILES -->>
	To spool the record or data is the way to solve this problem
		spool /home/oracle/Documents/spool/student_spool.csv;
		set heading off;
		set feedback off;
		set termout off;
		set pages 30 lines 200;
		set colsep ",";      -- if this line not used then tab will be field seperator
		prompt Student Details;
		prompt --------------------;
		prompt -----------------------------------------;
		select * from sandesh.student;
		set heading on;
		set feedback on;
		set termout on;
		spool off;
		
WITH CLAUSE -->>
	Usage of subqueries can be minimized
	If there are many levels of subqueries then the performance will drastically deteriorate
	If there are more than one exactly same subquery used in a query then all query will take time to execute
		This is solved with 'with' clause in which the subquery will only be computed once
	Usage
		select * from student where id in (select 9720 from dual);

		with val as (select 9720 val_column from dual)
		select * from student where id in (select val_column from val); 
		
CONNECT BY CLAUSE -->>
	Used in hierarchical queries
	(Primary key or Unique key) and foreign key column is used for this; no any constraint on both column also works but their must be some relation
	Top - Down hierarchy and Bottom - Up hierarchy
	Top-Down		PRIOR PK = FK | FK = PRIOR PK
	Bottom-Up		PRIOR FK = PK | PK = PRIOR FK
	Usage 
		for table employee with (employee_id pk column and manager_id fk column self referencing employee_id)
		select employee_id, last_name, job_id, manager_id, level
		from employees
		start with employee_id = 101	-- either at a top or bottom determined by prior keyword
		connect by prior manager_id = employee_id;   -- here employee_id is pk so it is bottom-up hierarchy (101 will be at the bottom of the hierarchy)
			-- i.e, 101 will have the level <last>
		
		select employee_id, last_name, job_id, manager_id, level
		from employees
		start with employee_id = 101	-- either at a top or bottom determined by prior keyword
		connect by prior employee_id = manager_id;   -- here employee_id is pk so it is top-down hierarchy (101 will be at the top of the hierarchy)
			-- i.e, 101 will have the level 1
		
MATERIALIZED VIEW -->>
	Database view that stores pre computed results
	Like the result of complex group by, complex aggregate , complex joins result
	For remote database objects we create materialized view in local that's the main purpose
		Without this the performance of whole business will be worst
		Refresh can be done in non peak hour
	Create materialize view is required
	Base table must have a primary key constraint
	If database link is being used then create database link is also required
	Types of mat view
		Normal
			create materialized view view_name
			build [immediate | deferred]    -- immediate is the default ; with deferred the data will be populated only after first refresh
			refresh [fast | complete | force]
			on [commit | demand]   -- instead of these time interval can also be provided
			[[enable | disable} query rewrite]  -- disable in default
				-- with this enabled all the aggregation and complex task is precomputed beforehand and during refresh data is inserted quickly
			as select ....;
		Pre-Built
			create materialized view view_name
			on prebuilt table
				-- with prebuilt option, name of mat view and base table name must be same
				-- if the base table is already referenced by other mat view then also it will throw error
			refresh [fast | complete | force]
			on [commit | demand]
			[[enable | disable} query rewrite]
			as select ....;
	Types of refresh
		complete - Truncate and insert fresh copy from base table (takes longer time)
		fast - mat view log file should be present, otherwise it fails; if has more than one base table then log file should be there for all tables
				for shorter refresh interval fast refresh is used
				to create mat log - NOTE: create materialized view log on student;
		force - fast refresh is attempted, if fails then complete refresh is done, log file will be read for the any changes made and the change
				is applied
	Types or refresh events
		commit - refresh happens on every commits on base table
		demand - execute dbms_mview.refresh('student_mat_view'); -- executing this
				in general - exec dbms_mview.refresh('view_name', 'c') -- c means complete refresh
		periodically - providing the time in mat view definition
	Usage
		create materialized view view_name 
		as
		statement;
		
		create materialized view student_mat_view
		as
		select * from student where id <= 5000;    -- complex query should be written, easy just for demo
			-- If the table student is dropped now, then also the record in student_mat_view still persists unlike view
			-- If the table student changes then also the change won't get reflect in the mat view
			-- Mat view should be updated manually once the base table changes
			
		create materialized view mv_emp
		refresh complete			-- whole data will be refreshed
		start with sysdate         -- refresh starts from today
		nex trunc(sysdate) + 1		-- next refresh is after 1 day (i.e, each day it will refresh)
		as select ............;
	Alter
		alter materialized view view_name refresh complete;
		alter materialized view view_name refresh complete start with sysdate;
		alter materialized view view_name refresh on demand complete;
		
		
------------------------------------------------------------ ORACLE PL/SQL -------------------------------------

PL/SQL -->>
	It is a procedural language i.e, we can write step by step process in it
	It is case insensitive prog. language
	It is platform independent
	It is not ANSI standard
	It uses blocks
		Each block makes a single request to db server
	select * from user_source;  -- this contains all the code required for the build of plsql procedure
	ARCHITECTURE
		PLSQL BLOCK is divided into procedural statements and SQL statements
			Procedural statements are executed in PL/SQL engine
			SQL statements in Oracle server
		PLSQL block architecture
			DECLARE -- optional section; contains variables, constants, cursors, exception, pragma, type, local subprograms etc.
				-- local subprograms must at the end during declaration or PRAGMA can also be at the end
			BEGIN	-- Mandatory section (procedural statements + sql statements)
			EXCEPTION -- optional section (procedural statements + sql statements)
			END;
		Types of block
			Nameless block (Anonymous block)
			Named block
				Named block are stored in db in compiled form
	DBMS_OUTPUT.PUT_LINE -- for printing
	DBMS_OUTPUT.PUT -- for printing in single line
	EXECUTE DBMS_OUTPUT.ENABLE
	EXECUTE DBMS_OUTPUT.DISABLE
	Before printing the message serveroutput should be enabled
		set serveroutput on;
		show serveroutput;
		
		set autoprint on; -- this will print the value of variable automatically without once the plsql block is compiled
	All variable declared inside declare section are local variables..
	To use the global variable bind variable can be used while calling from plsql
	Global variables are also called, host, environment and global variable.
	
DBMS_OUTPUT -->>
	ENABLE			--DBMS_OUTPUT.PUT.ENABLE;
	DISABLE			--DBMS_OUTPUT.DISABLE;
	PUT				--DBMS_OUTPUT.PUT('');
	PUT_LINE		--DBMS_OUTPUT.PUT_LINE('');
	NEW_LINE		--DBMS_OUTPUT.NEW_LINE();
	GET_LINE		--DBMS_OUTPUT.GET_LINE(input varchar out, int status out);    -- no need to use them  -- input is from previous put_line
	GET_LINES		--DBMS_OUTPUT.GET_LINES(inputs varchar out, int statuses out);  -- no need to use them -- input is previous multiple put_line
	
	
ANONYMOUS BLOCK -->>
	Useful
		declare
			some_var varchar2(100) := 'Some dummy value written';
			some_const number default 3.14;
		begin
			some_const := 7;
			dbms_output.put_line(some_var || ' and PI: ' || some_const);
		end;
		/
		
	Using bind variable
		variable person_name varchar2(20);
		execute :person_name := 'Sandesh Pokhrel';

		begin
			dbms_output.put_line('Value of bind variable person_name is: ' || :person_name);
		end;
		/
			
	Using type of table column
		declare
			first_name employees.first_name%type;   -- %type and %rowtype are two plsql attributes
			last_name employees.last_name%type;
			v_query varchar2(1000);
		begin
			v_query := 'select first_name, last_name from employees where employee_id = 100';
			select first_name, last_name into first_name, last_name from employees where employee_id = 100;
			dbms_output.put_line('Full name is: ' || first_name || ' ' || last_name);
			
			execute immediate v_query into first_name, last_name;
			dbms_output.put_line('Full name is: ' || first_name || ' ' || last_name);
		end;
		/
		
		declare
			employee_row employees%rowtype;    -- here a table row type is used instead of individuals
		begin
			select first_name, last_name into employee_row.first_name, employee_row.last_name from employees where employee_id = 100;
			select * into employee_row from employees where employee_id = 100;
			dbms_output.put_line('Full name is: ' || employee_row.first_name || ' ' || employee_row.last_name);
		end;
		/
		

	set transaction read only;   -- transaction mode modified  ; this should be the first line of transaction otherwise error
	set transaction read write;  -- default mode ;  this should be the first line of transaction otherwise error
	TCL and DML can be used inside plsql.
	DDL and DCL can be used but should be used dynamically.
	
	Attributes
		Two attributes in plsql
			%type
			%rowtype
			
			
NAMED BLOCK -->>
	Lables can be given anywhere; not only above begin; it's like a marker to indicate for goto
	Nested Nameless block
		declare
			some_integer number := 100;
		begin
			dbms_output.put_line('From outside: ' || some_integer);
			begin
				dbms_output.put_line('From inside: ' || some_integer);
			end;
		end;
		/

	Nested Named block
		declare
			some_integer number := 100;
		begin
			dbms_output.put_line('From outside: ' || some_integer);
			<<XYZ>>
			begin
				dbms_output.put_line('From inside (Named Block) : ' || some_integer);
			end;
		end;
		/
		
	Named block
		<<ABC>>
		declare
			some_integer number := 100;
		begin
			dbms_output.put_line('From outside: ' || some_integer);
			<<XYZ>>
			begin
				dbms_output.put_line('From inside (Named Block) : ' || some_integer);
			end XYZ;
		end;
		/
		
		Loop like implementation using labels
		declare
			v number := 1;
		begin
			dbms_output.put_line('Start: ' || v);
			<<XYZ>>
			dbms_output.put_line('Inside XYZ: ' || v);
			if v>= 5 then
				return;
			end if;
			v := v + 1;
			goto XYZ;
		end;
		/
		
IDENTIFIER -->>
	Max length 30 character
	Allowed characters A-Z,a-z,0-9,_,$,#
	First char must be an alphabet
	Not case sensitive
	Keywords are not allowed as identifiers
	
	
CONTROL STRUCTURES -->>
	Conditional - if-elsif if-else if-elsif-else.. end if;
	Iterative - simple loop: (finite, infinite), while loop, for loop: (incremental, decremental)
	Sequential - goto, null
	
	Relational operators
		<, >, <>, !=, >=, <=, AND, OR
		
	Simple loop
		Infinite
			loop
				......
			end loop;
			
		Finite
			loop
				....
				exit when condition;  -- if condition never matches then also infinite
				OR
				if v_count = 5 then
					exit;
				end if;
			end loop;
			
	While loop
		while con loop
			.....
		end loop;
		
	For loop
		FOR loop_counter IN [REVERSE] lowest_number..highest_number LOOP
		   {...statements...}
		END LOOP;
		
	Goto
		Move the execution control to particular labels
		Move upward or downward within begin particular
		Move upward or downward within exception part
		It cannot move from begin to exception or vice versa
		Cannot move from if condition to outside and vice versa
		Cannot move to other blocks
		
	Null
		It will do nothing
		Hust passes the execution control to next statement
		Usage
			begin
				if 1=1 then
					null;
				end if;
				dbms_output.put_line('After');
			end;
			/
			
OPERATORS -->>
	:= assignment
	.. range
	~=, <>, !=, ^= inequality -- ~= only works in pl/sql
	** exponentation
	=> association
		-- exec proc_name(5); OR exec_proc_name(proc_arg => 5);
		
		
SUBPROGRAMS -->>
	Two types
		PROCEDURE
			Mainly to perform an action
			Allows all DML + TCL statements
			Accepts all modes of parameters
			Accepts in, out, in out parameters
			Donot return any value directly
			Return values through out/in out parameters
			Cannot have return clause
			Cannot be called in the DML statements
			Return statement is optional in the body
			
			create or replace procedure p_name 
			is
			........ (optional)
			begin
			.... (mandatory)
			end;
			/
			
			execute p_name; OR exec p_name();
			
		FUNCTION
			Mainly to perform an calculations
			Allows all DML + TCL statements
			Accepts all modes of parameters
			
			create or replace function f_name return number  --  return type can be anything 
			is
			........ (optional)
			begin
			.... (mandatory)
			end;
			/
			
			exec dbms_output.put_line(f_name); OR exec dbms_output.put_line(f_name());
			select f_name from dual;
			
		PARAMETERS
			IN -> Assignment cannot be performed with the parameter indicated as IN
			OUT -> Value cannot be passed as argument for OUT, if passed then the value will be null automatically
				-- bind variable can be passed as an out param
			IN OUT -> value can be passed as well as it is returned
			
			IN, OUT, IN_OUT, Default args with default keyword and default args with assignment operator
			Function is same, the difference is it can also return the value 
			create or replace procedure proc_with_params 
			(ids in number, returned out number, in_return in out number, default_val in number default 57, def_assign_val in number := 99)
			is
			begin
				dbms_output.put_line('Passed returned is: ' || returned);
				dbms_output.put_line('Passed in + returned is: ' || in_return);
				dbms_output.put_line('Passed id is: ' || ids);
				returned := 200;
				in_return := 700;
				dbms_output.put_line('Modified returned is: ' || returned);
			end;
			/

			variable out_var number;
			variable in_out_var number;
			execute :out_var := 300;
			execute :in_out_var := 200;
			execute proc_with_params(returned => :out_var, in_return => :in_out_var, ids => 100);
			execute dbms_output.put_line(:out_var);
			execute dbms_output.put_line(:in_out_var);
			
			
			NOTE: In case of function if it has out parameter then it cannot be called using select and any other DML statements
					Functions with TCL cannot be called through DML statements
					Function with DML write cannot be called through DML statements
					
PACKAGES -->>
	A database object
	Stored in database
	It has two parts - Specification and Body
	Package spec and body must have the same name
	Package spec is mandatory. Pacakge body may or may not be present. Body cannot be compiled without its specs.
	All the subprograms declared inside the spec must present in the body.
	Body can contain subprograms that is not declared in the spec, those subprograms are local and can only be utilized from within the package.
	Package is a collection of subprograms (procedures + functions)
	There can be zero subprograms in the package. This is like orphan package.Body is not required for this package.
	
	Usage
		-- This is an orphan package. As no subprograms are declare it does not need a body.
		create or replace package orphan_package is
			v_num number := 10;
			cursor c1 is select * from employees;
		end orphan_package;
		/
		execute dbms_output.put_line (orphan_package.v_num);
		
		
		create or replace package emp_package as
			v_global_unlike_in_body number;		-- This is a global variable. Accessed from all subprograms and from the outside also.
			function func_in_package (nums in out number ) return number;
			procedure proc_in_package (nums in out number);
		end emp_package;
		/

		prompt create or replace package body emp_package				-- This is used for printing something in the screen
		create or replace package body emp_package as
			v_global number;		-- Declaring global variable for the whole package's subprograms. Can't be accessed from outside unlike that of specs.
			function func_in_package (nums in out number) return number is
			begin
				nums := nums**2;
				return nums;
			end func_in_package;
			
			-- This procedure is not in the package specification
			-- This is called local procedure as it is accessible for only the package inside this procedure
			-- Declare this procedure above the one which is using it. Otherwise the package won't compile.
			procedure local_procedure is
			begin
				dbms_output.put_line('Local procedure called');
			end local_procedure;

			procedure proc_in_package (nums in out number) is
			begin
				nums := nums*2;
				dbms_output.put_line('Num * 2 = ' || nums);
			end proc_in_package;
			
			-- Initialization block inside the package; This is optional; It must not contain the end statement
			-- This is executed for the first time of the session only
			begin
				dbms_output.put_line('Welcome');
		end emp_package;
		/

		variable var_bind number;
		execute :var_bind := 5;
		execute emp_package.proc_in_package(:var_bind);
		print :var_bind;

		execute dbms_output.put_line ('Printing from function: ' || emp_package.func_in_package(:var_bind));
		print :var_bind;
	
	
			
					
------------------------------------------------------------ DATABASE ADMINISTRATION -------------------------------------
		
ARCHITECTURE -->>
	Made up of three parts - 
		Oracle instance
		Oracle database storage
		Oracle server processes (SPs)
		RDBMS
			Instance
				Process
					PMON
					SMON
					DbWn
					Ckpt
					LGWR
					ARCn
					Optional (ARCn, ASMB, RBAL, Others)
				Memort Structure
					SGA
						Data buffer cache
						Shared pool
						Redo log buffer
						Large pool
						Java pool
						Streams pool
					PGA
			Database
				Data files
				Control files
				Redo log files
				Archived redo log files
				Parameter files
				Password file
				Backup files
				Trace files
				Alert log file
				
				
BACKGROUND PROCESSES -->>
	DATABASE WRITER - 
		Writes data from data buffer cache to the disk
		Dirty  buffer = modified
		Cold buffer = not recently
		Writes the both types of buffers to the disk, such the buffer gets free space for other data.
		Can have more than one database writer according to the need.
		DB_WRITER_PROCESSES parameter can be used to configure more writers.
		When ? -- When a server process cannot find a clean reusable buffer, and when oracle needs to advance the checkpoint i.e,
			the oldest dirty buffer from which the database need to recover during failure i.e, in redo log file
			
	LOG WRITER -
		Redo log buffer is the cyclic buffer, 3 in count
		Responsible for writing from redo log buffer to redo log files
		When ? --
			When a user process commits a transaction
			When the redo log buffer is one-third full
			Before a DBWn process writes modified buffers to disk
			Every 3 seconds
			
	CHECKPOINT PROCESS -
		Checkpoint is a database event which synchronizes the modified data blocks in memory with the data files on disk
		It also updates the datafile header and control file with the latest checkpoint System Change Number (SCN)
		When ? --
			At each switch of the redo log files
			Once the number os seconds defined in the LOG_CHECKPOINT_TIMEOUT is reached
			Once the current redo log file reaches the size
				LOG_CHECKPOINT_INTERVAL * size of IO OS blocks
			Directly by the ALTER SYSTEM SWITCH LOGFILE command
			Directly with the ALTER SYSTEM CHECKPOINT command
			
	SMON (System Monitor) PROCESS -
		Performs recovery at the instance startup
		Clears the temporary unused segment
		
	PMON (Process Monitor) PROCESS - 
		Performs process recovery when a user process fails
			Cleans up the buffer cache
			Free resource that are used by the user process
		Restarts stopped running dispatchers and server processes
		Dynamically registers database service with network listeners
		
	RECO (Recoverer) PROCESS - 
		Used with the distributed database configuration
		Automatically resolves all in-doubt transactions
		
	ARCn (Archiver) PROCESS -
		Copy redo log files to a designated storage device after a log switch occurs
		Runs only in ARCHIVELOG mode
		LOG_ARCHIVE_MAX_PROCESS param to configure the number of archiver process
		
		
MEMORY STRUCTURES -->>
	SGA (Shared Global Area) 
		Program code (PL/SQL code, Java code)
		Cached data shared among users
		Redo log entries
		Information about currently connected sessions etc.
		This is shared across multiple users
		
		BUFFER CACHE -
			holds the copies of data blocks that are read from data files
			shared by all concurrent users
			number of buffers defined by DB_BLOCK_BUFFERS
			size of a buffer based on DB_BLOCK_SIZE
			stores the most recently used blocks
			Data must first come in buffer before the user can read it
			
		SHARED POOL -
			Library cache
				Contains statement text, parsed code and an execution plan, Contains one or more sql area
				Shared SQL area
					contains a parse tree and execution plan for a given SQL statement
			Data dictionary cache
				Contains table and column definitions and privileges
			Result cache
				It stores the result of sql query and the plsql query
				If same query is run multiple times then it gives back the same result
			Size of shared pool SHARED_POOL_SIZE param
			
		REDO LOG BUFFER -
			This is a circular buffer which stores the changes made to the database
			DML, DDLs etc..
			Useful during database recovery
			Size is defined by LOG_BUFFER
			
		LARGE POOL -
			This is an optional memory area which provide large memory allocations
			If not defined then will reduce performance as the shared pool is used instead
			Parallel query operations
			Oracle backup and restore operations
			Make heavy operations use large pool instead of shared pool
			
		JAVA POOL and STREAMS POOL - 
			Used to store java code and java object required for java virtual machine
			Size defined by JAVA_POOL_SIZE
			
			Used to Oracle streams to store buffered queue message and provide memory for oracle streams processes
			Size defined by STREAMS_POOL_SIZE
			
		KEEP BUFFER POOL -
			Data which is frequently accessed can be pinned in the keep buffer pool
			In case if buffer cache is full and the data is to be removed will cause the problem if that data is highly used
			To create keep buffer pool DB_KEEP_CACHE_SIZE
			
		RECYCLE BUFFER POOL -
			Recycle pool is reserved for large tables that experience full table scans that are unlikely to be reread or accessed rarely
			Size defined by DB_RECYCLE_CACHE_SIZE
			
		nK BUFFER CACHE -
			Is used to store oracle data blocks having different size then the default block size (8192k)
			Default block size is denoted by DB_BLOCK_SIZE
			Size defined by DB_NK_CACHE_SIZE
			Eg: DB_2K_CACHE_SIZE = 0M, DB_4K_CACHE_SIZE = 0M etc ...
			
		
	PGA (Program Global Area)
		This area of memory is not shared.
		Users have their own PGA to work in their session
		PGA contains private SQL area for each server processes for each user connection
			That private SQL will be stored in Shared SQL area inside library cache checking if all are running similary query
		Its a private memory region containing data and control information for a server process
		
		
DEDICATED VS SHARED SERVERS -->>
	Dedicated servers -
		One server process is spawned for each user connection
		Consumes lots of memory
		Greater performance
	Shared servers -
		Server processes are shared among multiple connections
		It provides scalability
		Here first the user request is handled through Dispatcher process and is stored in the request & response queues. Then from that shared process performs its actions.
		Nobody is using these servers nowadays.
		In shared servers PGA contains only stack space and SGA contains User global areas. Both are in PGA in case of dedicated servers.
		
ORACLE INSTANCE -->>
	Is loaded into the memory each time database starts (In memory nature)
	Contains shared memory caches (Shared pool, buffer cache, redo buffer etc.)
	Contains Oracle's background processes
	Instance will be removed from the memory once it is shut down (non persistent nature)
	One instance is mounted to only one database at a time.
		In 12c multi tenant architecture many pluggable database are there for single container database. 
	
DATABASE STORAGE -->>
	This is the storage which contains oracle files to store the data
	This makes the physical oracle database
	Microsoft word and microsoft word file is analogous to oracle instance and oracle file
	Data storage is usually - Both are natively supported by oracle
		SAN -  Storage Area Netword (Accessed by fibre network)
		NAS -  Netword Attached Storage 
	Data can also be place in local disk (not recommended - single point of failure)	
		
		
SERVER PROCESSES (SPs) -->>
	Server processes are spawned when user is trying to connect to database.
	Each user will have the dedicated server process unless pooling is used with the help of middleware.
	It's role -
		verify the syntax of sql query that the user is trying to execute
		it then read data from disk and also load it in the buffer cache
		
		
LOGICAL AND PHYSICAL DATA STRUCTURES -->>
	Database (L) - Tablespace (L) - Data Files (P) - Segment (L) - Extent (L) - Data block (L)
	Tablespace can contain one or more data files, whereas one datafile can be mapped to only one tablespace
	
	DATA BLOCK -
		A data block is the smallest unit of storage in an Oracle database
		Common and variable header
			It contains the address of block, general block info, type of segment eg. data or index segment
		Table directory
			Info about table having the row in the block
		Row directory
			Info about the row for which the data is stored
		Row data 
			Actual area in which the data is stored
		Free space
		
	TABLESPACE -
		Three types Permanent, UNDO and Temporary
			Permanent for storing info like tables, indexes etc...
			UNDO for the rollback and data consistency during select
			Temporary for storing the temp data like of join, order by etc.
		DEFAULT tablespaces	
			SYSTEM		- Manage the database, data dictionary and other adminstrative objects, cannot take offline, can store user data but strict no
			SYSAUX		- Introduced after 10g, auxillary to system, some of the stuffs stored in system are now in sysaux table space, cannot take offline
			TEMP		- Temporary table space, used to store temporary data, sort , merge, joins etc
			UNDOTBS1	- Undo data
			USERS		- Used to store all the data created by users by default if the tablespace for that user is not allocated
			EXAMPLE		- This schema is created if the scripts file is ran during oracle installation, sample schemas
			
		Online and Offline tablespace -
			Users can connect and read data from is tablespace is online
			Users cannot read write if tablespace is offline, moving a datafile, recover datafile and tablespace, offline tablespace backup etc.
			Tablespace as default will be in online state
			
		Useful -
			select * from dba_data_files;
			select * from dba_tablespaces;
			select tablespace_name, bytes/1024/1024 blocks_in_mb from dba_free_space where tablespace_name = 'TBS1';
			select tablespace_name, file_name, bytes/1024/1024 blocks_in_mb from dba_data_files;
			
			create tablespace tbs1 datafile '/mnt/san_storage/oradata/orcl/data01.dbf' size 10m autoextend on reuse next 512k maxsize 200M;
				autoextend on to increase the size if the initial size is full
				next 512k is to increase the datafile size by 512 if data file is full
				maxsize is the size till which the tablespace can be extended upto
				reuse option to use the datafile if exists
			create tablespace tbs1 datafile '/mnt/san_storage/oradata/orcl/data01.dbf' size 1m;
			alter tablespace tbs1 add datafile '/mnt/san_storage/oradata/orcl/data02.dbf' size 5m;  -- adding datafile
			alter tablespace tbs1 drop datafile '/mnt/san_storage/oradata/orcl/data02.dbf';  -- dropping datafile
			drop tablespace tbs1;  -- dropping the tablespace
			drop tablespace tbs1 including contents and datafiles; -- including datafiles and contents
			alter tablespace tbs1 rename to tbs2;  --renaming the tablespace
			
			-- for renaming the datafile first take tablespace to offline
			alter tablespace tbs1 offline;
			alter database rename file '/mnt/san_storage/oradata/orcl/data01.dbf' to '/mnt/san_storage/oradata/orcl/data11.dbf';
				for this, first rename data01.dbf to data11.dbf in filesystem using mv command and then run this command otherwise error
			alter tablespace tbs1 online;
			
			if datafile is full - either of the three
				alter database datafile '/mnt/san_storage/oradata/orcl/data01.dbf' resize 5m;   -- resizing
				alter tablespace tbs1 add datafile '/mnt/san_storage/oradata/orcl/data02.dbf' size 5m; -- adding
				create tablespace tbs1 datafile '/mnt/san_storage/oradata/orcl/data01.dbf' size 10m autoextend on; -- create tablespace with autoextend
			
			Tablespace with different block size
				Bigger size needed
					Larger tables that are the target for full table scans
					table with larger objects (LOB's, BLOB's, CLOB's)
					temporary tablespace for sorting
					tables with large rows that might lead to chained/migrated rows
				show parameter db_block_size;
				alter system set db_16k_cache_size=60m scope=both; -- both means spfile and memory
				show parameter db_16k_cache_size;
				create tablespace tbs2 datafile '/mnt/san_storage/oradata/orcl/data01.dbf' size 10m blocksize 16k;	
		
		TEMPORARY TABLESPACE MANAGEMENT
			Used during joins, sorts and other temporary operations
			select * from dba_temp_files;
			select tablespace_name, file_name, bytes/1024/1024 blocks_in_mb from dba_temp_files;
			alter database tempfile '/mnt/san_storage/oradata/orcl/temp01.dbf' resize 140m; -- resizing default temp tablespace
			create temporary tablespace temp2 tempfile '/mnt/san_storage/oradata/orcl/temp02.dbf' size 20m autoextend on next 1m maxsize 60m;
			select * from database_properties;  -- this will display various database configuration stuffs like defualt temp tablespaces etc..
			alter database default temporary tablespace temp2;  -- sets defualt temp tablespace as temp02
			
		TEMPORARY TABLESPACE GROUP
			Multiple tablespaces can be assigned to a group such that those groups can be assigned to the users
			Group will be create while first tablespace is added and removed when the last tablespace is removed from the group
			create temporary tablespace temp2 tempfile '/mnt/san_storage/oradata/orcl/temp02.dbf' size 20m autoextend on next 1m maxsize 60m
				tablespace group tempgroup1 ; -- cannot be used with other types of tablespaces
			select * from dba_tablespace_groups; -- lists all the groups
			alter tablespace temp2 tablespace group tempgroup1;
			select * from database_properties;
			alter database default temporary tablespace tempgroup1;  -- sets defualt temp tablespace as the group of temp tablespaces
			
		LOCALLY VS DICTIONARY MANAGED TABLESPACE
			In dictionary managed tablespace, all the extent related info is stored in the dictionary for the tablespace
				This is slower approach as the dictionary table is to be queried to fetch the information
				create tablespace tbs1 datafile '/mnt/san_storage/oradata/orcl/data01.dbf' size 10m autoextend on next 512k maxsize 200M
					extent management dictionary default storage (initial 50k next 50k minextents 2 maxextents 50); -- only for 10g and before
					
			In dictionary managed tablespace, all the extent related info is stored within the tablespace itself
				create tablespace tbs1 datafile '/mnt/san_storage/oradata/orcl/data01.dbf' size 10m autoextend on next 512k maxsize 200M
					extent management local autoallocate; -- must use locally managed 11g onwards
					
				create tablespace tbs1 datafile '/mnt/san_storage/oradata/orcl/data01.dbf' size 10m autoextend on next 512k maxsize 200M
					extent management local uniform size 128k; -- must use locally managed 11g onwards
					
					
		UNDO DATA
			It is the copy of pre modified data that is captured for every transaction that changes data.
			Why ?
				Rollback transactions
				Support read consistent queries
				Support flashback operations
				Recover from failed transactions
			How long does undo data stay ?
				User commits the transaction
				User rollbacks the transaction
				User execute DDL statements (create, drop, alter, rename)
				User session terminated abnormally (transaction rollbacks)
				User session terminates normally with an exit (transaciton commits)
			Where does undo data stay ?
				Stored in undo segments/undo tablespace
				Only 1 active tablespace for an instance
				They are owned by the user sys
			With 11g, automatic undo management is the default mode
				UNDO_MANAGEMENT = AUTO / MANUAL; show parameter undo_management;
			An auto extended UNDO tablespace named UNDOTBS1 is automatically created while creating database with DBCA.
			SYSTEM tablespace is used , if no UNDO tablespace is defined.
			UNDO retention period is managed very effectively. UNDO retention period is the number of seconds the committed undo data is retained.
			Only one undo table sapce can be online at a time
			Useful
				alter tablespace undotbs add datafile '/mnt/san_storage/oradata/orcl/undotbs01.dbf' size 5m;  -- adding datafile
				create undo tablespace undotbs02 datafile '/mnt/san_storage/oradata/orcl/undotbs02.dbf' size 10m reuse autoextend on next 512k maxsize 200M;
					reuse option is used to use the datafile if already present
				alter system set undo_tablespace=undotbs2; -- changing the used undo tablespace
				select segment_name, owner, tablespace_name, status from dba_rollback_segs;
			CONFIGURING UNDO RETENTION PERIOD
				UNDO_RETENTION - 1800
				ALTER SYSTEM SET UNDO_RETENTION = 2400
				After a transaction is committed, undo data is no longer needed for rollback. But still it is persisted for consistent read purposes.
				Furthermore, the success of several oracle flashback feature also depend upon the availability of older undo information
				Useful
					show parameter undo_retention;
					alter system set undo_retention=2400;
			RETENTION GUARANTEE
				This guarantee that the data exists for the specified amount of time (UNDO_RETENTION) in the undo tablespace
				If the space is not available in undo tablespace then it will fails any other transaction but still persist the data in the tablespace.
				alter tablespace undotbs1 retention guarantee; -- guarantee the retention (needs the space)
				alter tablespace undotbs1 retention noguarantee; -- doesnot guarantee
				select tablespace_name, retention from dba_tablespaces;  -- check for the retention policy
				
REDO CONCEPTS -->>
	Redo log files enable the oracle server or DBA to redo transactions if a database failure occurs
	Only purpose is to enable recovery
	Redo log entries from buffer is stored in the redo log files	
	Management
		Stores redo log files as a group, in which one group contains many redo log files stored in different disk
		The data is written in circular fashion
		Current log file
			The redo log file to which LGWR is actively writing
		Active log file
			Log files required for instance recovery
			Active log files cannot be overwritten by LGWR until ARCn has archived the data when archiving is enabled
		Inactive log file
			Log files no needed for instance recovery
	Useful
		select * from v$logfile;
		select * from v$log;
		alter database add logfile member '/mnt/san_storage/oradata/orcl/redo01b.log' to group 1;
		alter database add logfile member '/mnt/san_storage/oradata/orcl/redo02b.log' to group 2;
		alter database add logfile member '/mnt/san_storage/oradata/orcl/redo03b.log' to group 3;
			now each group has 2 log files
			by default their status will be invalid as the logwriter has not used them and will be changed once used
			Manually we can switch the logfiles to see the result quickly.   -- alter system switch logfile;
		alter database drop logfile group 3;  -- group 3 log files are deleted, for this the happen the group should be in inactive status
		alter database add logfile group 3 ('/mnt/san_storage/oradata/orcl/redo03.log', '/mnt/san_storage/oradata/orcl/redo03b.log') size 10m reuse;
			add group and the logfile within them dissimilar with the above way of addition in which we already have the group
			reuse option is used to use the datafile if already present
			
	REDO LOGS ARCHIVAL
		Redo log files enable the oracle server to redo transactions if a db failure occurs
		ARCn process is responsible for archiving redo log files
		ARCn process only works if the database has archive log mode enabled
		Useful
			archive log list;   -- to view the log mode of the database
			shutdown immediate;
			startup mount;
			alter database noarchivelog;  // alter database archivelog;
			alter database open;
			USE_DB_RECOVERY_FILE_DEST as archive destination points to the fast recovery area.
			select thread#, sequence#, name from v$archived_log; -- view archived logs
			
			
USER MANAGEMENT -->>
	Database user account is a way to organize the ownership of database objects and access rights.
	Each user account has
		A unique username,
		An authentication method
		A temporary tablespace
		A default tablespace
		Account status
		Initial consumer group
		User profile
	Default administrative accounts 
		SYS - all privileges, can perform anything, owns database dictionary
		SYSTEM - powerful account, cannot perform backup and recovery, cannot perform database upgrade
		DBSNMP - Used by oracle enterprise manager (graphical tool provide by oracle for performing admin tasks) to monitor the database
		SYSMAN - Used by oracle enterprise manager to perform administrative tasks
	Useful
		select * from session_privs;   -- to check the privileges
		create user user_name identified by password;
		create user user_name identified by password password expire;
		create user john profile default identified by password profile default default tablespace USERS temporary tablespace TEMP account unlock;
		grant connect to user_name/role;
		grant create table to user_name [with admin option]; -- with admin option guarantees that the created user can grant that privilege to others
		alter user user_name quota 5m on tblsp_name;  -- altering the quota on the tablespace
		grant select any table to user_name; -- 'any' clause means the user can select tables from all schema
		select * from dba_ts_quotas;   -- check the space of user in tab
		lespace
		
		GOTO ---create user and permissions--- SECTION FOR MORE QUERIES
		
	SYSTEM VS OBJECT PRIVILEGE - 
		Object privilege is the action that are allowed to be operated on certain users.
			SELECT, UPDATE, DELETE, ALTER, EXECUTE
			grant <object_privilege> on <object> to <grantee clause> -- general
			grant delete on john.student to tom [with grant option]; -- perform delete on student table of john to tom user
			........ [with grant option] --- privilege will cascade on revoke
		System privilege are others studied
			......[with admin option] -- with grant option is only for object privilege
			on revoking the privilege, the revoke won't cascade in case of [with admin option] for system privilege
			
	USER PROFILE - 
		Profiles are used to set limits on database resource
		Each user can have only one profile and is by default set to 'default' profile
		Profiles are used to control resource consumption, manage account status and password expiration
		Resources
			CPU, Disk, Connect Time, Idle Time, Concurrent sessions, Private SGA, etc....
		Useful
			select * from dba_profiles;    -- list of profiles and the list of resource names
			select * from dba_users;  -- list of users and the profile they are in
			create profile profile_name limit 
				sessions_per_user 2       -- can open two sessions max
				idle_time 5					-- can remain idle for 5 min then session is closed
				connect_time 10;			-- can stay connected for 10 min then session is closed
			create profile password_prf limit
				password_life_time 180
				password_grace_time 7
				password_reuse_time unlimited
				password_reuse_max unlimited
				failed_login_attempts 10
				password_lock_time 1
				password_verify_function verify_function_11g;    -- many more parameters to set
				
				
NETWORKING CONCEPTS -->>
	Oracle net service runs boths in the client and server.
	When a user tries connecting to the database then the request first goes through the net service running in client and then to the listener.
		client (oracle net) ---- listener ---- server (oracle net)
	Once the connection is established, then the user process will directly interact with the server process without listener.
	listener.ora file is present in the oracle server which has the info regarding the network related configuration parameters.
		this file must contain protocol, host and port number. Service name is optional and should be listed if multiple services are running
	Listener can be configured using following approaches
		enterprise manager, oracle net manager, oracle net configuration assistant, command line
		'netca' command can be used to run oracle net configuration assistant
	tnsnames.ora file is used to bundle up the host, protocol, port and service name information such that those parameter can be replaced with the name during connection
		we can directly add the tns entry inside tnsnames.ora file
		'netmgr' tool can be launched which also let us add the entry graphically
		
		
DATABASE LINKS -->>
	Database link or dblink is the schema object which let us connect to the other database remotely
		eg: from local branch to the headquarters host
	Other database need not be some, it can be two different databases
	create database link dblink_name
		connect to schema_name_to_connect_to identified by password_name_of_that_remote_schema
		using 'tns_entry_name';       -- creating the database link, this is private database link
	select * from table_name@dblink; -- selecting from the remote database
	select * from dba_db_links;  -- selecting the dblinks present in the database
	create public database link dblink_name
		connect to schema_name_to_connect_to identified by password_name_of_that_remote_schema
		using 'tns_entry_name';  -- public database link, can be used by anybody on that database
	We must have database backup before the time in which we are restoring the database
		and the archived redo log for the period between the SCN of the backups and the target SCN (System Change Number)
	
		
DATA DICTIONARY -->>
	----Check in basic section-----
	

------------------------------------------------------------ DATABASE BACKUP AND RECOVERY -------------------------------------	

COMPLETE VS INCOMPLETE RECOVERY -->>
	Restore to a previous point in time (PITR). ---incomplete
	Restore to a current point in time. --complete
	
POINT IN TIME RECOVERY (PITR) -->>
	Combining the full backup taken at certain period of time + the archived redo log files after the backup will restore the database.
	Archive log mode should be enabled
		the archives are stored in DB_RECOVERY_FILE_DEST parameter size defined by DB_RECOVERY_FILE_DEST_SIZE

	DATABASE FAILURE CATEGORIES -
		Human failure
			resolved by flashback and oracle data pump
		Machine failure (instance failure but healthy database)
			HA technologies and oracle RAC
		Disk failure (healthy instance but failure in database)
			Oracle RMAN and hot backups
		Data center failure (whole data center failure)
			DR (Disaster Recovery) technologies and oracle data guard
			
	Physical vs Logical data backups
		Physical
			Copying the files like Data files, Control files, Archived redo logs to other locations
			Oracle RMAN (Recovery Manager) is used for physical backups
		Logical
			Exporting database objects like tables, indexes, etc..
			Oracle data pump is used for this
			
	Oracle backup technologies
		User-managed hot backups
			Manually copy oracle data files to a seperate location to create a physical backup of the database
			Can be done while database is running
			Incremental backups cannot be done with this approach
			Not used that much at recent time
		Oracle RMAN
			Comprehensive framework for physical backup and recovery
			Invoked using rman command-line utility
			supports incremental backups
			Creates bulletproof backups
		Data pump
			Enables portability of data and metadata from one db to another
			support filtering during export/import
			Suppoert interchangeability between different versions
		Flashback
			provides way to view past states of the entire database or specific objects
			Usually invoked through 'flashback' command
			
	Enable archivelog mode
		show con_name;  -- make sure to be connected to container database
		alter pluggable database all open;  -- to open all pluggable database after alter database open;
		goto ---redo sections---	
		
	Fast recovery area	
		Select * from v$database;
		Disk or file system available to manage the backup or log files
		It is oracle managed directory or file system
		Usually stores archived redo logs, flashback logs, RMAN backups
		The archives are stored in DB_RECOVERY_FILE_DEST parameter size defined by DB_RECOVERY_FILE_DEST_SIZE
		
	USER MANAGED BACKUPS
		Manually copying oracle data files to a seperate location
		start by enabling the database online 'backup mode'   -- alter database start backup;
		finish by taking the database out of 'backup mode'		-- alter database end backup;
			i)  Then perform copy commands and copy the required data files etc
				If the storage device is enterprise level oracle filesystem then various useful operation are available like cloning etc.
			ii) Now the backup of control files and archived logs.
				control file backup ----  alter database backup controlfile to 'controlfile_backup_destination/control_file_backup.ctl'
				archive redo log backup ---- alter system archive log current;
												select thread#, sequence#, name from v$archived_log;
			iii) Performing backup  --
				use operating system utility to perform the backup
				first copy the backup taken previously (datafiles, controlfiles and archived redo logs)
				after that the backup is done using the recover utility of sqlplus
				steps
					shutdown immediate;
					startup mount;
					set autorecovery on;
					show parameter log_archive_format;  -- format of the archive redo files
					recover database;   -- displays no recovery required message if no datafiles are corrupted and no recovery is needed.
					alter database open;
					
	ORACLE RMAN (Recovery Manager)
		RMAN is the utility recommended by oracle than others methodology.
		RMAN is used both for backup and the restoration
		By default stored in fast recovery area
		RMAN command line is used for everything instead of other hectic os commands.
		Performs online database backup
		Supports two types of backups - full and incremental backups
			full - backing up everything
			incremental - copy changed data blocks
			levels of incremental backup
				level 0 - similar to the full backup
				level 1 - cumulative and deferential, two types
		Backup made by RMAN is called backup set, and can only be restored using RMAN
		Useful
			rman target=/; -- target = <sid> -- / = default instance
			rman target=sys/sagarmatha@orcl
				now rman is opened like sqlplus RMAN>
				show all; -- show all the RMAN parameters -- its result are given below
					using target database control file instead of recovery catalog
					RMAN configuration parameters for database with db_unique_name ORCL are:
					CONFIGURE RETENTION POLICY TO REDUNDANCY 1; # default    -- how many level 0 backups should RMAN retain
					CONFIGURE BACKUP OPTIMIZATION OFF; # default
					CONFIGURE DEFAULT DEVICE TYPE TO DISK; # default    -- by default store in a disk rather than tapes etc.
					CONFIGURE CONTROLFILE AUTOBACKUP ON; # default		-- instruct to backup control and sp file during backup
					CONFIGURE CONTROLFILE AUTOBACKUP FORMAT FOR DEVICE TYPE DISK TO '%F'; # default
					CONFIGURE DEVICE TYPE DISK PARALLELISM 1 BACKUP TYPE TO BACKUPSET; # default
					CONFIGURE DATAFILE BACKUP COPIES FOR DEVICE TYPE DISK TO 1; # default
					CONFIGURE ARCHIVELOG BACKUP COPIES FOR DEVICE TYPE DISK TO 1; # default
					CONFIGURE MAXSETSIZE TO UNLIMITED; # default
					CONFIGURE ENCRYPTION FOR DATABASE OFF; # default
					CONFIGURE ENCRYPTION ALGORITHM 'AES128'; # default
					CONFIGURE COMPRESSION ALGORITHM 'BASIC' AS OF RELEASE 'DEFAULT' OPTIMIZE FOR LOAD TRUE ; # default
					CONFIGURE RMAN OUTPUT TO KEEP FOR 7 DAYS; # default
					CONFIGURE ARCHIVELOG DELETION POLICY TO NONE; # default
					CONFIGURE SNAPSHOT CONTROLFILE NAME TO '/u01/app/oracle/product/12.2.0/dbhome_1/dbs/snapcf_orcl.f'; # default
					
				command for manually configuring those params	
					CONFIGURE RETENTION POLICY TO REDUNDANCY 1;
					CONFIGURE RETENTION POLICY TO RECOVERY WINDOW OF 7 DAYS;  -- retain backup for 7 days and delete older
					
				Useful	
					backup database plus archivelog;  -- instructs RMAN to take backup of everything
					backup database tag 'DAILY_BACKUP' plus archivelog;  -- use tags to identify different backups
					backup database root;  -- only backup the root (CDB), don't backup any PDBs
					backup pluggable database plg_db_name;  -- only backup pluggable db with name plg_db_name
					if connected to pluggable database
						rman plg_db_name/password@service
						backup database plus archivelog; -- will only backup the connected pdb
					Incremental
						backup incremental level 0 database tag 'inc_level0' plus archivelog;
						backup incremental level 1 database tag 'inc_level1' plus archivelog; -- least amount of data block backed up, differentiating
						backup incremental level 1 cumulative database tag 'inc_level1' plus archivelog;
							cumulative back ignores all other backups and only cares about last full backup
							it takes more space than differential backup
							it takes more time to execute
							it should not analyze more backup sets
		---STUDY RMAN AGAIN---					
							
							
	ORACLE DATA PUMP						
		Data pump is done using the utility 'expdp'
		Useful
			Export
				expdp system/sagarmatha schemas=sandesh dumpfile=mydump01.dmp; -- file is dumped in the default oracle directory
				expdp system/sagarmatha schemas=<comma seperated list of schemas> dumpfile=mydump01.dmp; -- exporting from multiple schemas
				create directory my_exp_dir1 as '/u01/app/oracle/admin/orcl/my_export_directory'; -- will not create file system dir if not exists
				grant read,write on directory my_exp_dir1 to public;
				expdp system/sagarmatha schemas=sandesh dumpfile=mydump01.dmp directory=my_exp_dir1; -- exporting to the directory created
				expdp system/sagarmatha schemas=sandesh dumpfile=mydump01.dmp directory=my_exp_dir1 logfile=mylog01.log; -- with the log file
					-- logfile is the exact mirror of the messages during the expdp command run
				
				alter session set container=cdb$root; -- setting container database as the connected one
				create pluggable database demo_PDB admin user pdb_main identified by pdb_admin file_name_convert('/u01/app/oracle/oradata/orcl12c/pdbseed/',
					'/u01/app/oracle/oradata/orcl12c/demo_pdb/'); -- creating a pluggable database
				alter pluggable database demo_pdb open;  -- opening the pluggable database;
				alter session set container=demo_pdb;
				expdp system/sagarmatha@localhost:1521/demo_pdb directory=directory_name dumpfile=demo_full.dmp; -- export content of complete pdb
			Import
				The exported file does not depend upon the oracle version, once exported can be imported by any other system
				impdp system/sagarmatha@localhost:1521/orcl directory=my_exp_dir1 dumpfile=sandesh01.dmp logfile=sandesh01_IMPORT.log;
					-- if the data already exist then it will throw the error stating data is already present
				impdp system/sagarmatha@localhost:1521/orcl directory=my_exp_dir1 dumpfile=sandesh01.dmp logfile=sandesh01_IMPORT.log remap_schema=sandesh:success;
					-- remap schema will remap the content of (x:y) x schema obj in dump file to the y schema
				impdp system/sagarmatha@localhost:1521/orcl directory=my_exp_dir1 dumpfile=sandesh01.dmp logfile=sandesh01_IMPORT.log remap_schema=sandesh:success
					INCLUDE=USER,ROLE_GRANT,SYSTEM_GRANT,TABLE,INDEX;
					-- this specify the objects to include during import
					-- user grants, system grants are also applied to the user
				impdp system/sagarmatha@localhost:1521/orcl directory=my_exp_dir1 dumpfile=sandesh01.dmp logfile=sandesh01_IMPORT.log remap_schema=sandesh:success
					EXCLUDE=VIEW;
					-- exclude view and import everything
			Advanced object filtering
				expdp system/sagarmatha@localhost:1521/orcl schemas=sandesh dumpfile=mydump01.dmp directory=my_exp_dir1 logfile=mylog01.log
					INCLUDE=TABLE:\""IN ('MY_FIRST_TABLE','ANOTHER_TABLE')\"", VIEW:\""IN ('MY_FIRST_VIEW')\"";
					-- double quotation is used to escape how the linux file system treats double quotes
					-- using parameter file will solve this problem
				expdp system/sagarmatha@localhost:1521/orcl parfile=/home/Documents/params.par
					-- providing parameter file as an argument and all other necessary arguments are passed via param file
			Monitoring and Controlling
				expdp system/sagarmatha schemas=sandesh dumpfile=mydump01.dmp directory=my_exp_dir1 logfile=mylog01.log full=y;
					-- full=y refers that this export is going to run for sufficiently long period of time
				select * from dba_datapump_jobs; -- tracking the jobs that are running as export or import data pump
				expdp system/sagarmatha@localhost:1521/orcl attach=SYS_EXPORT_FULL_01
					-- SYS_EXPORT_FULL_01 this is the running datapump job_name. dba_datapump_jobs table has the job info and name
					-- entering above command will open the expdp prompt attached to that job
					-- this provides the various info regarding the job
					-- 'kill_job' command in expdp prompt will terminate the job
					
	ORACLE FLASHBACK
		With flashback we can be at the previous timeline of database with the single command.
		Flashback doesnot work in case of DDL statements like altering table structure etc.
		If whole table is dropped then the recycle bin feature can be used
		Requirements
			Flashback query	
				Uses data stored in undo tablespace so requires the proper undo retention policy
				select * from student as of timestamp to_timestamp('13-DEC-19 02.28.32.771409000 PM'); -- data at time stamp before modification
				create table old_student as select * from student as of timestamp to_timestamp('13-DEC-19 02.28.32.771409000 PM');  -- restoring old data
				flashback table student to timestamp (systimestamp - interval '60' second);  
					-- flashback table before 60 second
					-- if row movement is not enabled then this query will throw error
					alter table student enable row movement; -- altering student table to enable row movement
						-- row id is not preserved by flashback query to enabling row movement in the table will solve this problem
			Flashback database
				Uses flashback logs stored in FRA
				To enable to flashback database
					shutdown immediate;
					startup mount;
					archive log list;  -- make sure that database is running with archive log mode enabled
						-- logs used by flashback database is also stored in db_recovery_file_dest;
							db_recovery_file_dest is also used by archived redo logs and RMAN backup
					show parameter db_flashback_retention_target;  -- length of time flashback database supports
					alter database flashback on;
					alter database open;
					alter pluggable database all open;
					
				To restore using flashback database
					shutdown immediate;
					startup mount;
					flashback database to timestamp to_timestamp ('timestamp','format');
						OR
					flashback database to timestamp to_timestamp ('timestamp');
						OR
					SCN (System change number) can also be used for flashing back the database
					alter database open resetlogs;
						-- resetlogs or noresetlogs must be used
					
		If whole table is dropped then the recycle bin feature can be used
			show parameter recyclebin; -- this should be on
			flashback table student to before drop;  -- recreate the table from before drop operation
			select * from dba_recyclebin;   -- view the content of recycle bin
			select * from user_recyclebin;   -- view the content of recycle bin from users
			purge recyclebin;  -- clearing recyclebin (user specific; if ran with sys then also remove object belonging to sys)
			purge dba_recyclebin; -- clears everything from recyclebin
			drop table student purge; -- purge will make sure that the object won't move to recycle bin once deleted
			
		Creating restore point
			create restore point DB_IS_OK guarantee flashback database;  -- running from root will create the point for all pdbs and cdb a restore point for the flashback database
			select * from v$restore_point; -- table containing SCN, restore point name and timestamp to restore the database to
			flashback database to restore point DB_IS_OK; -- DB_IS_OK is restore point name
			
			
			
------------------------------------------------------------ ORACLE 12C  -------------------------------------		

MULTITENANT ARCHITECTURE -->>
	Runs multiple oracle databases under the same Oracle instance
	Makes running multiple Oracle databases on a single server more cost effective and easier to manage
	Database consolidation and multitenant architecture
	Simple management and self service
	Rapid provisioning of new databases
	Manage multiple DBs as a single DB
	Dedicated pre 12c servers 
		Assigning single server to single application
			i.e, we have single dedicated server containing of instance, resources and database for each application
		More servers to manage
		DR and HA - complex  (Disaster Recovery and High Availability)
		More licenses to buy
		Cannot share resource easily in case workload on one is greater than another
		More servers = more CPU cores = more money to spend on
	Pre 12c instance consolidations
		Single application will have a dedicated database and instance but other resources will be shared among them
		If one application is added then one database should also be added alongside with the instance
		Movement of one database to another server is really complex procedure to follow
	Pre 12c schema consolidations	
		One server, running one database but has many schemas per application running
		Logically seperate but not physically seperates as schemas are simply logical objects
		If same application ran by two different branches then cloning schema twice will create lot of trouble like in public synonyms etc.
		Moving one schema to another server is also difficult as they are not physically seperated
		Logical copy technology like data pump should be used which is no fast comparatively
	With 12c
		Single server, single instance, multiple databases
		Each application will have each database
		Backup, restoration, migration is often easier
		Contains CDB (Container Database) and PDB (Pluggable Database)
			CDB acts as a container to the many pluggable databases
			CDB can be called as a super database with many sub (pluggable) databases
			CDB contains pluggable databases (PDBs) and a root container which contains the set of oracle provided objects etc..
			
	Architecture
		Contains a container database
		Container database consists of root container, a seed pdb and zero or many PDBs
		Container db has the container id of 0
		Root container contains redo logs, control files, archived logs, flashback and all other tablespaces etc.. Root container has the container id of 1
		Seed pdb is the type of pdb which is used to create other pd. seed pdb has the container id of 2. It does not serve any application wise purpose.
			It is named PDB$SEED
		Undo tablespace is only contained in root container
		System, sysaux and temp is present in alls PDBs too
		Root container is named CDB$ROOT with container id 1
		Upto 253 pluggable databases can be present
		Each PDB will have its own server processes as the SPs is spawned for each user connection
			Buffer cache will now track, which PDB does that block belong to
			
	connect 
		show con_name;  -- print the container in which we are connected to	
		sqlplus sys/sagarmatha@localhost:1521/orcl as sysdba; -- here orcl is the cdb name	
		sqlplus my_pdb_admin/password@localhost:1521/my_new_pdb as sysdba; -- to the newly created pdb
	create
		create pluggable database my_new_pdb
		admin user my_pdb_admin identified by password   -- admin user for that pdb
		ROLES = (dba)       -- granting dba roles for the above user, otherwise user won't have any privilege
		DEFAULT TABLESPACE my_tbs  -- default tablespace to use
		DATAFILE '/oradata/orcl/my_new_pdb/mytbs01.dbf' size 50m autoextend on
		file_name_convert=('/oradata/orcl/pdbseed', '/oradata/orcl/my_new_pdb')  -- clone from the seed pdb;
	alter
		alter pluggable database my_new_pdb open; -- opening the created pdb or closed pdb
		alter session set container=me_new_pdb; -- to change the container without logging out and connecting to the new one
		alter pluggable database my_new_pdb open;
		drop pluggable database my_new_pdb <including datafiles / keep datafiles>;
	useful	
		select * from v$pdbs;
		select * from v$container;
	pdb portability
		alter pluggable database my_new_pdb close <immediate>;
		alter pluggable database my_new_pdb unplug into '/storage/pdb.xml'; -- this xml file is called the manifest file, containing the metadata regarding original db
		drop pluggable database my_new_pdb keep datafiles;
		create pluggable database my_new_pdb_replugged  using '/storage/pdb.xml' nocopy tempfile reuse; -- reuse the tempfile and donot copy anything i.e, use all datafiles
		alter pluggable database my_new_pdb_replugged open;
	pdb cloning
		create pluggable database source_pdb
		admin user my_pdb_admin identified by password
		ROLES = (dba)
		DEFAULT TABLESPACE my_tbs
		DATAFILE '/oradata/orcl/my_new_pdb/mytbs01.dbf' size 50m autoextend on
		file_name_convert=('/oradata/orcl/pdbseed', '/oradata/orcl/my_new_pdb');
		alter pluggable database source_pdb open;
		alter pluggable database source_pdb close;
		alter pluggable database source_pdb open read only; -- must be in read only state
		create pluggable database target_pdb from source_pdb file_name_convert('/oradata/orcl/source_pdb/', /oradata/orcl/target_pdb/';
		alter pluggable database target_pdb open;
	data dictionary view
		cdb_,dba_,all_ and user_ dictionary views are present now
		Each pdb has - dba_,all_ and user_ views
		Root container has - cdb_ and all others views
		cdb_ - all objects in all containers
		dba_ - all objects in the database
		all_ - all objects in the db which the user has access to
		user_ - all objects owned by the user
	local and common user
		local users are the local to the database created
		common users are for all databases i.e, cdb and pdbs
		common user created can connect to all the database if the privilege is given
		common user should be prefixed with c##.
		Default scope of grant is only to the pdb from which the grant is performed.
		local user is created using the admin user of the pdb to connect.
			from my_new_pdb >
				create user username identified by password;
		common user is create from root container with sys
			from cdb$root >
				create user c##username identified by passsword;
				grant create session to c##username; -- i.e, the user can only connect to the db from which the grant is performd. in this case cdb$root
				grant create session to c##username container=all; -- now can connect to all containers
	

------------------------------------------------------------ MIGRATING NON CDB TO PDB  -------------------------------------

Steps for Conversion. 

Step 1.  Cleanly Shutdown the Non-CDB Database Instance. 

          ==- set environment to NCDB

          ==- sqlplus / as sysdba

          ==- Shutdown immediate 

Step 2.  Once You Have Cleanly Shutdown the Database, Start Up the database in Mount Exclusive Mode and Open the Database in Read-Only Mode. 

          ==- set environment to NCDB

          ==- sqlplus / as sysdba        

          ==- startup mount exclusive

          ==- alter database open read only; 

Step 3. Generate a Pluggable Database Manifest File from the Non-Container Database. 

          ==- set environment to NCDB

          ==- sqlplus / as sysdba        

          ==- exec dbms_pdb.describe (pdb_descr_file=-'E:\app\oracle\manifest\NCDB_manifest_file.xml'); 

Step 4. Shutdown the NON-CDB file Once Step 3 Completes Successfully. 

          ==- Set environment to NCDB

          ==- sqlplus / as sysdba

          ==- shutdown immediate 

Step 5. Start the CDB (CDB) if its Not Already Up and Check the Compatibility with CDB.

          ==- set environment to CDB

          ==- sqlplus / as sysdba

          ==- startup (If not up)

          ==- Run below at SQL prompt. 

          SET SERVEROUTPUT ON;

          DECLARE

          Compatible CONSTANT VARCHAR2(3) :=CASE  DBMS_PDB.CHECK_PLUG_COMPATIBILITY 

          (pdb_descr_file =- 'E:\app\oracle\manifest\NCDB_manifest_file.xml')

          WHEN TRUE THEN 'YES'

          ELSE 'NO'

          END;

          BEGIN

          DBMS_OUTPUT.PUT_LINE(compatible);

          END;

          / 

Step 6. Once it Completes Successfully, Query PDB_PLUG_IN_VIOLATIONS View from CDB Database for Any Errors.

          ==- set environment to CDB

          ==- sqlplus / as sysdba

          ==- select name, cause, type, message, status from PDB_PLUG_IN_VIOLATIONS where

                   name='NCDB';  

Note: "There should be no violations reported. If there are any, you need to fix it before proceeding". 

Step 7. Connect to the CDB Where Database has to be Plugged in Using the Noncdb Manifest File and Plug the PDDB12C Database.

          ==- set environment to CDB

          ==- sqlplus / as sysdba         

          ==- CREATE PLUGGABLE DATABASE NCDB USING 'E:\app\oracle\manifest\NCDB_manifest_file.xml'

          COPY

          FILE_NAME_CONVERT = ('E:\APP\ORACLE\ORADATA\NCDB\', 'E:\app\oracle\oradata\CDB1\'); 

          Note: "Below options are supported and you can chose one based on the env" 

1) COPY: The datafiles of noncdb remains intact and it is copied to create PDBs at new locations and keep original datafiles intact at the original location. (This would mean that a noncdb database would still be operational after the creation of a PDB).

2) MOVE: The datafiles of noncdb are moved to a new location to create a PDB. In this case, noncdb database would not be available after a PDB is created.

NOCOPY: The datafiles of noncdb are used to create a PDB2 and it uses same existing location. In this case, a noncdb database would not be available after a PDB is created. 
You can use FILE_NAME_CONVERT parameter to specify the new location of the datafiles while using COPY or MOVE option. 

Step 8. Once Step 7 Completes Successfully, Switch to the PDB Container and Run the "$ORACLE_HOME/rdbms/admin/noncdb_to_pdb.sql".

          ==- set environment to CDB

          ==- sqlplus / as sysdba

          ==- alter session set container=NCDB

          ==- @$ORACLE_HOME/rdbms/admin/noncdb_to_pdb.sql 

Step 9.  Startup the PDB and Check the Open Mode.

          ==- set environment to CDB

          ==- sqlplus / as sysdba

          ==- ALTER PLUGGABLE DATABASE OPEN;

          ==-SELECT name, open_mode FROM v$pdbs;

		
------------------------------------------------------------ ORACLE INSTALLATION -------------------------------------
	Steps:
		i) Edit /etc/hosts file and add 	127.0.0.1	<eg:oel7.lab.local>	<eg:oel7>
			this is to resolve domain from ip and viceversa, run hostname command to resolve what to put in second parameter

		ii) Resolve oracle kernel parameters as well as the group that is to be added for the oracle functioning like 
					groupadd -g oinstall, groupadd -g dba, useradd -g oinstall -G dba oracle passwd oracle... This all will be done by below command
				yum install oracle-rdbms-server-12cR1-preinstall
				yum install oracle-database-server-12cR2-preinstall
			group and user are stored in /etc/group and /etc/passwd file respectively. command -->> id oracle --- will also display
		iii) Now create directory where oracle is to be installed. For this login as root user and enter commands
			mkdir -p /u01/app/oracle    --- oracle binary file will be stored here i.e, oracle software
			chown -R oracle:oinstall /u01
			chmod -R 775 /u01

			mkdir -p /mnt/san_storage/oradata --- oracle datafiles, redofiles etc will be stored here i.e, datafiles
		iv) Now change /home/oracle/.bash_profile
			export ORACLE_HOSTNAME=oel7.lab.local i.e, hostname
			export ORACLE_UNQNAME=orcl
			export ORACLE_BASE=/u01/app/oracle
			export ORACLE_HOME=$ORACLE_BASE/product/12.1.0.2/dbhome_1
			export ORACLE_SID=orcl
			export PATH=/usr/sbin:$PATH
			export PATH=$ORACLE_HOME/bin:$PATH

		v) Now set SELINUX to be permissive, this will print warning instead of enforcing security
			/etc/selinux/config     SELINUX=permissive
			run command -->> setenforce permissive ---- to immediately show the effect otherwise restart to see the change
		vi) If network firewalls are up and running stop them so they don't interfere
			service iptables stop
			chkconfig iptables off
		vii) Create the folder where oracle is to be downloaded
			With root user - 
				mkdir -p /orasoft
				chown -R oracle:oinstall /orasoft

	After installation --
		Creating a database instance
			Goto dbhome/bin folder and run "netca"
			Go on and add the listener, default port of 1521 can be used as the listener's port
			After finishing it -
				run 'lsnrctl' command, after that 'status' command will show what listener is currently running
		Creating a database
			Database can also be created in manual for which we have to create everything like spfile (parameters) in dbs folder,
				database itself (
					create database orcl
					darafile '..../example.dbf' size 300m autoextend on
					sysaux datafile '............' ......... on
					default tablespace user_data datafile '............' 
					undo tablespace undotbs datafile '.....................'
					logfile group 1 '......./redo1.redo' size 100m,
							group 2 '......../redo2.redo' size 100m;
				) save this  sql in some location, then connect to idle instance of database with admin and then startup nomount
				  after this run that sql file and all other sql files, 
				datafiles etc...
				This method is hectic , so prefer below options
			Goto dbhome/bin folder and run "dbca"       //// In case of upgrade run "dbua"
			While creating database select advanced mode and do accordingly
			For simplicity Create as container database is not checked, if checked then pluggable database will be created too if selected accordingly
			Oracle enterprise manager is the web interface provided through which we can manage our database as administrator
			During the setup of recovery area checking the archiving will archive the redo logs automatically
			Storage area can be setup which must be the secured one
			After filling up all the required details, the database creation and the instance creation step will begin.
		view /etc/oratab file for all the services running in the host
			orcl:/u01/app/oracle/product/12.2.0/dbhome_1:N, here N means the instance will not run automatically after reboot
			
	
		
		
		
---------------- ORACLE LINUX FIXING SWAP SIZE ----------------------------------------------------------------

	1 check what swap size
	#swapon s
	Filename                                Type            Size    Used    Priority
	/dev/xvda3                              partition       2097144 0       -1
	  
	2 Create a file that youll use for swap with dd command with   as root
	dd if=/dev/zero of={/swapfile path} bs={size of swap}  count=1048576
	Example:-
	[root@localhost]#dd if=/dev/zero of=/home/swapfile bs=6048 count=1048576
	Note :- Above command will be create the 6Gb swapfile on /home location as  swapfile
	3 Set up a Linux swap area
	#mkswap /home/swapfile
	4: Enabling the swap file
	#swapon /home/swapfile
	#swapon a
	5 status of add swap
	[root@myrem12c em12cBP1]# swapon -s
	Filename                                Type            Size    Used    Priority
	/dev/xvda3                              partition       2097144 0       -1
	/home/swapfile                          file            6097144 0       -2
	6 Update /etc/fstab   (this must to be done )
	#vi /etc/fstab
	/home/swapfile        none             swap   sw      0 0  
	Note  :- Add above line ending with  the file otherwise ones you restart the server swap partition not be mounted to the system

	Then retry the installation
		
		
		
---------------------- USEFUL LINUX COMMANDS (CENTOS) --------------------------------	
	
	wget http://get.geo.opera.com/pub/opera/linux/1216/opera-12.16-1860.x86_64.rpm
	lsnrctl status;	lsnrctl start;	lsnrctl stop;
	lsnrctl > status; lsnrctl > start; lsnrctl > stop;
	via sqlplus >
		start and stop the instance -
			alter system register;   >> force instance to connect to the listener automatically
			shutdown - nomount - mount - open
			startup
				nomount - 
					only instance is started, not connected to the database, used for database creation and recovery purpose
				mount -
					taking the instance and associating with the database
					only the dba can access database at this stage, no user can access, still closed
					for enabling and disabling archive log mode
					for renaming data files etc.
					users cannot access the database at this stage
					adding, dropping, renamind redo log files
					performing database recovery
				open -
					all users can access the database
			shutdown
				immediate -
					terminate all executing sql query, and disconnects the user
					uncomitted changes are rollbacked
					issues the checkpoints and closes all the databases
				transactional -
					waits for all the transactions to complete
					prevents user from starting new transactions and creating sessions
					oracle then perform checkpoint, disconnects all the users and close
				normal -
					wait for all transactions to complete
					can start new transactions by the connected user but doesnot allow new user to connect
					coloses the database at the end after performing checkpoint
				abort -
					everything will be stopped
					instance recovery should be performed later

		parameters related -
			show parameter sga_target;
			show parameter sga_; >> list all the params with sg_ text as substring
			alter system set sga_target=1000m scope=memory/spfile/both;  >> memory -- cleared after restart, spfile -- stored in spfile reflects after restart, both -- both
			create pfile='/home/oracle/my_pfile.ora' from spfile;    >> this will copy all the parameters from spfile so that we can view it
			Parameter file are of two type :
				pfile and spfile, pfile is stored as string whereas spfile is stored as binary
				pfile -> $ORACLE_HOME/dbs/init.ora
				spfile - > $ORACLE_HOME/dbs/spfile[sid].ora
		create user and permissions -
			create user user_name identified by password;   >> if connected then displays create session privilege error
			grant create session to user_name;    >> if create table is executed - insufficient privilege
			grant unlimited tablespace, create table to user_name; 
			drop user user_name;   >> only if the schema does not owns anything otherwise
			drop user user_name cascade;
			create role role_name;
			grant (roles.....) to role_name;
			grant role_name to user_name;
		data dictionary -
			v$logfile;    >> view all redo log files
			dba_data_files;    >> view data files, <file_name, tablespace_name> shows the datafile belonging to tablespace
			v$controlfile;		>> view control file.  >> if multiplexing is enabled then it will be stored in multiple places
			v$session;
			dba_tablespaces;
		troubleshoot using alert log -
			It contains all warnings, errors generated by oracle
			cd $ORACLE_BASE/diag/rdbms/orcl/orcl/trace   >> here alert_orcl.log
		storage - 
			tablespace is a collection of datafiles used as the storage
			dba_tablespaces -
				system - it is the tablespace where oracle data dictionary are stored.
			dba_data_files;    >> view data files, <file_name, tablespace_name> shows the datafile belonging to tablespace	
			create tablespace tbspc_name datafile '/mnt/san_storage/oradata/dfile01.dbf' size 20m autoextend on;
			drop tablespace tbspc_name including contents and datafiles;
		control files -
			The database control file maintains information about the physical structure of the database
			The control file is critical to recovering a data file because the control maintains the System Control Number (SNC) for each data file
			Multiplexing control file will eradicate the single point of failure issue
			CREATE CONTROLFILE is used to manually create a controlfile
			ALTER DATABASE BACKUP CONTROL FILE TO TRACE for backup
		
		
		
		
	